<Section: TRAINING>
Logging to /tmp/openai-2020-09-30-13-08-56-728322
Creating dummy env object to get spaces
(None, None)
self.navi_state.shape (4096,)
/usr/local/lib/python3.8/dist-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
(None, None)
self.navi_state.shape (4096,)
reset
cur_obs [ 25   0   8   0   0  30 833 123  33   0   6   5]
max_step 26 num_step 0
reward 0
(48,) 0 False {}
cur_obs [ 24   5   6  25   8  60 714 154  68  24   0  40]
max_step 26 num_step 1
reward 0
(48,) 0 False {}
cur_obs [ 32  17  11  27   1  90 625 155  88  18   1 113]
max_step 26 num_step 2
reward 0
(48,) 0 False {}
cur_obs [ 49   4   5  22   0 120 570 136  80  15   3 196]
max_step 26 num_step 3
reward 0
(48,) 0 False {}
cur_obs [ 28   3   6   5   0 150 535 122  42  13   3 285]
max_step 26 num_step 4
reward 0
(48,) 0 False {}
cur_obs [  8   0   3   5  16 180 496  81  32  12   1 378]
max_step 26 num_step 5
reward 0
(48,) 0 False {}
cur_obs [ 12   0   2   2  11 210 454  76  27   0   2 441]
max_step 26 num_step 6
reward 0
(48,) 0 False {}
cur_obs [  0   3   5   0   4 240 410  66  12  20   3 489]
max_step 26 num_step 7
reward 0
(48,) 0 False {}
cur_obs [  9   2   3   6   0 270 351  91  20   4   1 533]
max_step 26 num_step 8
reward 0
(48,) 0 False {}
cur_obs [  0   9   3  18   3 300 273  96  33  12   1 585]
max_step 26 num_step 9
reward 0
(48,) 0 False {}
cur_obs [ 45   1  11   2   0 330 205  95  59   2   5 634]
max_step 26 num_step 10
reward 0
(48,) 0 False {}
cur_obs [ 52   0  10   0   0 360 134  88  62   0  14 702]
max_step 26 num_step 11
reward 0
(48,) 0 False {}
cur_obs [ 53   0   4   0   0 390  93  62  57   0  31 757]
max_step 26 num_step 12
reward 0
(48,) 0 False {}
cur_obs [ 41   0   5   0   0 420  48  67  46   0  36 803]
max_step 26 num_step 13
reward 0
(48,) 0 False {}
cur_obs [ 31   0   7   0   0 450  30  44  38   0  41 847]
max_step 26 num_step 14
reward 0
(48,) 0 False {}
cur_obs [ 11   0   2   0   0 480  11  52  13   0  40 884]
max_step 26 num_step 15
reward 0
(48,) 0 False {}
cur_obs [  8   0   2   0   0 510   5  29  10   0  36 920]
max_step 26 num_step 16
reward 0
(48,) 0 False {}
cur_obs [  2   0   1   0   0 540   2  17   3   0  33 945]
max_step 26 num_step 17
reward 0
(48,) 0 False {}
cur_obs [  1   0   0   0   0 570   1   8   1   0  26 964]
max_step 26 num_step 18
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 600   0   4   0   0  24 972]
max_step 26 num_step 19
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 630   0   0   0   0  26 974]
max_step 26 num_step 20
reward 0
(48,) 0 False {}
cur_obs [  1   0   0   0   0 660   0   0   1   0  25 974]
max_step 26 num_step 21
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 690   0   1   0   0  25 974]
max_step 26 num_step 22
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 720   0   0   0   0  25 975]
max_step 26 num_step 23
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 750   0   0   0   0  25 975]
max_step 26 num_step 24
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 780   0   0   0   0  25 975]
max_step 26 num_step 25
reward 0
(48,) 0 True {'episode': {'r': 0}, 'events': [], 'env_id': 1}
reset
reset
cur_obs [ 14   0  11  18   0  30 833 119  43   0   1   4]
max_step 26 num_step 0
reward 0
(48,) 0 False {}
cur_obs [  0  10   7  17  15  60 714 168  49  36   0  33]
max_step 26 num_step 1
reward 0
(48,) 0 False {}
cur_obs [ 54  10   6   9   1  90 625 147  80  23   1 124]
max_step 26 num_step 2
reward 0
(48,) 0 False {}
cur_obs [ 54   3   3   5   0 120 570 149  65  14   1 201]
max_step 26 num_step 3
reward 0
(48,) 0 False {}
cur_obs [  0   0   9   2  21 150 535  97  32  28  13 295]
max_step 26 num_step 4
reward 0
(48,) 0 False {}
cur_obs [  0   0   8   0  20 180 496  57  28  18  27 374]
max_step 26 num_step 5
reward 0
(48,) 0 False {}
cur_obs [  0  14   5   0   8 210 454  83  27   0  11 425]
max_step 26 num_step 6
reward 0
(48,) 0 False {}
cur_obs [  0   0  10   0   7 240 410  67  17  13  28 465]
max_step 26 num_step 7
reward 0
(48,) 0 False {}
cur_obs [ 12   0   5  11   0 270 351  96  28   8   0 517]
max_step 26 num_step 8
reward 0
(48,) 0 False {}
cur_obs [ 26   0   0  16  12 300 273  86  54   9   0 578]
max_step 26 num_step 9
reward 0
(48,) 0 False {}
cur_obs [ 42   0  12   0   0 330 205  93  54   2   5 641]
max_step 26 num_step 10
reward 0
(48,) 0 False {}
cur_obs [ 52   0   9   0   0 360 134  83  61   0  19 703]
max_step 26 num_step 11
reward 0
(48,) 0 False {}
cur_obs [ 47   0   8   0   0 390  93  67  55   0  27 758]
max_step 26 num_step 12
reward 0
(48,) 0 False {}
cur_obs [ 37   0   4   0   0 420  48  69  41   0  36 806]
max_step 26 num_step 13
reward 0
(48,) 0 False {}
cur_obs [ 27   0   5   0   0 450  30  48  32   0  37 853]
max_step 26 num_step 14
reward 0
(48,) 0 False {}
cur_obs [ 13   0   3   0   0 480  11  46  16   0  43 884]
max_step 26 num_step 15
reward 0
(48,) 0 False {}
cur_obs [  3   0   4   0   0 510   5  27   7   0  45 916]
max_step 26 num_step 16
reward 0
(48,) 0 False {}
cur_obs [  6   0   1   0   0 540   2   9   7   0  39 943]
max_step 26 num_step 17
reward 0
(48,) 0 False {}
cur_obs [  4   0   1   0   0 570   1   9   5   0  31 954]
max_step 26 num_step 18
reward 0
(48,) 0 False {}
cur_obs [  3   0   0   0   0 600   0   5   3   0  29 963]
max_step 26 num_step 19
reward 0
(48,) 0 False {}
cur_obs [  1   0   0   0   0 630   0   2   1   0  26 971]
max_step 26 num_step 20
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 660   0   1   0   0  26 973]
max_step 26 num_step 21
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 690   0   0   0   0  26 974]
max_step 26 num_step 22
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 720   0   0   0   0  26 974]
max_step 26 num_step 23
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 750   0   0   0   0  26 974]
max_step 26 num_step 24
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 780   0   0   0   0  26 974]
max_step 26 num_step 25
reward 0
(48,) 0 True {'episode': {'r': 0}, 'events': [], 'env_id': 1}
reset
reset
cur_obs [  0   0   0  24   8  30 833 102  32  25   4   4]
max_step 26 num_step 0
reward 0
(48,) 0 False {}
cur_obs [  0   6  16  39   8  60 714 153  69  33   1  30]
max_step 26 num_step 1
reward 0
(48,) 0 False {}
cur_obs [ 66   4   0  31  12  90 625 142 113  22   2  96]
max_step 26 num_step 2
reward 0
(48,) 0 False {}
cur_obs [ 79   0   0  35  11 120 570 100 125  11  12 182]
max_step 26 num_step 3
reward 0
(48,) 0 False {}
cur_obs [ 43   5   6  11   0 150 535 127  65  11   1 261]
max_step 26 num_step 4
reward 0
(48,) 0 False {}
cur_obs [ 32   4   0  12  11 180 496  87  59  15   5 338]
max_step 26 num_step 5
reward 0
(48,) 0 False {}
cur_obs [  0   9   6   0   9 210 454 101  24   0   6 415]
max_step 26 num_step 6
reward 0
(48,) 0 False {}
cur_obs [ 12   0   0   4   3 240 410  89  19  14   3 465]
max_step 26 num_step 7
reward 0
(48,) 0 False {}
cur_obs [ 12   0   0  10   5 270 351  85  27  12   2 523]
max_step 26 num_step 8
reward 0
(48,) 0 False {}
cur_obs [ 25   0   0  10  10 300 273  84  45  15   3 580]
max_step 26 num_step 9
reward 0
(48,) 0 False {}
cur_obs [ 25   0   0  12   2 330 205  98  39  14   7 637]
max_step 26 num_step 10
reward 0
(48,) 0 False {}
cur_obs [ 25   0   0   8   3 360 134 105  36  19  10 696]
max_step 26 num_step 11
reward 0
(48,) 0 False {}
cur_obs [ 33   0  11   0   0 390  93  80  44   0  11 772]
max_step 26 num_step 12
reward 0
(48,) 0 False {}
cur_obs [ 31   0   5   0   0 420  48  57  36   0  21 838]
max_step 26 num_step 13
reward 0
(48,) 0 False {}
cur_obs [ 23   0   7   0   0 450  30  43  30   0  24 873]
max_step 26 num_step 14
reward 0
(48,) 0 False {}
cur_obs [  5   0   2   0   0 480  11  42   7   0  24 916]
max_step 26 num_step 15
reward 0
(48,) 0 False {}
cur_obs [ 11   0   2   0   0 510   5  19  13   0  21 942]
max_step 26 num_step 16
reward 0
(48,) 0 False {}
cur_obs [  0   0   1   0   0 540   2  16   1   0  18 963]
max_step 26 num_step 17
reward 0
(48,) 0 False {}
cur_obs [  3   0   0   0   0 570   1   8   3   0  16 972]
max_step 26 num_step 18
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 600   0   2   0   0  17 981]
max_step 26 num_step 19
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 630   0   0   0   0  17 983]
max_step 26 num_step 20
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 660   0   0   0   0  17 983]
max_step 26 num_step 21
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 690   0   0   0   0  17 983]
max_step 26(None, None)
self.navi_state.shape (4096,)
reset
cur_obs [ 16   6   0   0   2  30 833 120  24  21   0   2]
max_step 26 num_step 0
reward 0
(48,) 0 False {}
cur_obs [  0   9   8  20  11  60 714 170  48  40   0  28]
max_step 26 num_step 1
reward 0
(48,) 0 False {}
cur_obs [ 38  11   9  28   0  90 625 161  86  11   0 117]
max_step 26 num_step 2
reward 0
(48,) 0 False {}
cur_obs [ 49   3   7  22   0 120 570 138  81  12   2 197]
max_step 26 num_step 3
reward 0
(48,) 0 False {}
cur_obs [ 40   2   5  10   4 150 535 113  61  18   1 272]
max_step 26 num_step 4
reward 0
(48,) 0 False {}
cur_obs [  0   0   3   1  19 180 496  63  23  25  18 375]
max_step 26 num_step 5
reward 0
(48,) 0 False {}
cur_obs [ 13   0   3   5   7 210 454  77  28   0   3 438]
max_step 26 num_step 6
reward 0
(48,) 0 False {}
cur_obs [ 11   3   3   6   0 240 410  74  23   4   1 488]
max_step 26 num_step 7
reward 0
(48,) 0 False {}
cur_obs [ 10   3   6   5   0 270 351  78  24   8   0 539]
max_step 26 num_step 8
reward 0
(48,) 0 False {}
cur_obs [ 43   0  10   0   0 300 273  76  53   2   7 589]
max_step 26 num_step 9
reward 0
(48,) 0 False {}
cur_obs [ 56   0  10   0   0 330 205  77  66   0  20 632]
max_step 26 num_step 10
reward 0
(48,) 0 False {}
cur_obs [ 54   0   7   0   0 360 134  84  61   0  34 687]
max_step 26 num_step 11
reward 0
(48,) 0 False {}
cur_obs [ 52   0  10   0   0 390  93  63  62   0  40 742]
max_step 26 num_step 12
reward 0
(48,) 0 False {}
cur_obs [ 33   0   9   0   0 420  48  66  42   0  55 789]
max_step 26 num_step 13
reward 0
(48,) 0 False {}
cur_obs [ 30   0   7   0   0 450  30  46  37   0  55 832]
max_step 26 num_step 14
reward 0
(48,) 0 False {}
cur_obs [ 10   0   7   0   0 480  11  43  17   0  56 873]
max_step 26 num_step 15
reward 0
(48,) 0 False {}
cur_obs [  7   0   2   0   0 510   5  26   9   0  52 908]
max_step 26 num_step 16
reward 0
(48,) 0 False {}
cur_obs [  8   0   1   0   0 540   2  13   9   0  46 930]
max_step 26 num_step 17
reward 0
(48,) 0 False {}
cur_obs [  3   0   0   0   0 570   1  14   3   0  36 946]
max_step 26 num_step 18
reward 0
(48,) 0 False {}
cur_obs [  1   0   0   0   0 600   0   6   1   0  33 960]
max_step 26 num_step 19
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 630   0   5   0   0  30 965]
max_step 26 num_step 20
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 660   0   1   0   0  29 970]
max_step 26 num_step 21
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 690   0   0   0   0  29 971]
max_step 26 num_step 22
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 720   0   0   0   0  29 971]
max_step 26 num_step 23
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 750   0   0   0   0  29 971]
max_step 26 num_step 24
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 780   0   0   0   0  29 971]
max_step 26 num_step 25
reward 0
(48,) 0 True {'episode': {'r': 0}, 'events': [], 'env_id': 0}
reset
reset
cur_obs [  7   1   0   6   4  30 833 131  18  16   0   2]
max_step 26 num_step 0
reward 0
(48,) 0 False {}
cur_obs [  0  12   8  27  16  60 714 151  63  34   1  37]
max_step 26 num_step 1
reward 0
(48,) 0 False {}
cur_obs [ 55   4   0  32  20  90 625 128 111  27   2 107]
max_step 26 num_step 2
reward 0
(48,) 0 False {}
cur_obs [ 55   1   7  20   0 120 570 153  83   8   2 184]
max_step 26 num_step 3
reward 0
(48,) 0 False {}
cur_obs [  6   0  16   1  12 150 535 106  35  28  15 281]
max_step 26 num_step 4
reward 0
(48,) 0 False {}
cur_obs [  0   0   6   0   9 180 496  64  15  19  32 374]
max_step 26 num_step 5
reward 0
(48,) 0 False {}
cur_obs [  0   0   9   0  15 210 454  56  24   0  42 424]
max_step 26 num_step 6
reward 0
(48,) 0 False {}
cur_obs [ 11   0   1   6   2 240 410  88  20  10   2 470]
max_step 26 num_step 7
reward 0
(48,) 0 False {}
cur_obs [  8   5   2  11   0 270 351  77  26  10   0 536]
max_step 26 num_step 8
reward 0
(48,) 0 False {}
cur_obs [ 22   1   0   8   8 300 273  91  39  17   3 577]
max_step 26 num_step 9
reward 0
(48,) 0 False {}
cur_obs [ 29   0   0  18   2 330 205  89  49  15   4 638]
max_step 26 num_step 10
reward 0
(48,) 0 False {}
cur_obs [ 52   0  16   0   0 360 134  84  68   1  12 701]
max_step 26 num_step 11
reward 0
(48,) 0 False {}
cur_obs [ 43   0  14   0   0 390  93  61  57   0  20 769]
max_step 26 num_step 12
reward 0
(48,) 0 False {}
cur_obs [ 40   0   8   0   0 420  48  54  48   0  29 821]
max_step 26 num_step 13
reward 0
(48,) 0 False {}
cur_obs [ 22   0   0   0   0 450  30  49  22   0  37 862]
max_step 26 num_step 14
reward 0
(48,) 0 False {}
cur_obs [ 12   0   5   0   0 480  11  39  17   0  36 897]
max_step 26 num_step 15
reward 0
(48,) 0 False {}
cur_obs [  7   0   2   0   0 510   5  27   9   0  31 928]
max_step 26 num_step 16
reward 0
(48,) 0 False {}
cur_obs [  2   0   1   0   0 540   2  19   3   0  24 952]
max_step 26 num_step 17
reward 0
(48,) 0 False {}
cur_obs [  1   0   0   0   0 570   1   7   1   0  21 970]
max_step 26 num_step 18
reward 0
(48,) 0 False {}
cur_obs [  3   0   0   0   0 600   0   2   3   0  17 978]
max_step 26 num_step 19
reward 0
(48,) 0 False {}
cur_obs [  1   0   0   0   0 630   0   3   1   0  16 980]
max_step 26 num_step 20
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 660   0   0   0   0  18 982]
max_step 26 num_step 21
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 690   0   0   0   0  18 982]
max_step 26 num_step 22
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 720   0   0   0   0  18 982]
max_step 26 num_step 23
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 750   0   0   0   0  18 982]
max_step 26 num_step 24
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 780   0   0   0   0  18 982]
max_step 26 num_step 25
reward 0
(48,) 0 True {'episode': {'r': 0}, 'events': [], 'env_id': 0}
reset
reset
cur_obs [  0   4   0   0   9  30 833 107  13  41   5   1]
max_step 26 num_step 0
reward 0
(48,) 0 False {}
cur_obs [  0   9  18  16   6  60 714 173  49  42   1  21]
max_step 26 num_step 1
reward 0
(48,) 0 False {}
cur_obs [ 48   3   0  45  17  90 625 142 113  21   3  96]
max_step 26 num_step 2
reward 0
(48,) 0 False {}
cur_obs [ 50   2  12  36   0 120 570 140 100  12   2 176]
max_step 26 num_step 3
reward 0
(48,) 0 False {}
cur_obs [  0   2  13   0  18 150 535 123  33  26  18 265]
max_step 26 num_step 4
reward 0
(48,) 0 False {}
cur_obs [ 11   0   0  13   5 180 496  87  29  11   3 374]
max_step 26 num_step 5
reward 0
(48,) 0 False {}
cur_obs [  5   0   0   3  11 210 454  82  19   0   8 437]
max_step 26 num_step 6
reward 0
(48,) 0 False {}
cur_obs [ 15   0   4   8   0 240 410  69  27   6   3 485]
max_step 26 num_step 7
reward 0
(48,) 0 False {}
cur_obs [  8   4   3   4   0 270 351  88  19   5   0 537]
max_step 26 num_step 8
reward 0
(48,) 0 False {}
cur_obs [ 18   0   0   6  11 300 273  90  35  22   0 580]
max_step 26 num_step 9
reward 0
(48,) 0 False {}
cur_obs [ 30   0   0  11   0 330 205  92  41  14   5 643]
max_step 26 num_step 10
reward 0
(48,) 0 False {}
cur_obs [ 33   0   0  14   4 360 134  92  51  15   7 701]
max_step 26 num_step 11
reward 0
(48,) 0 False {}
cur_obs [ 27   0   7   0   0 390  93  94  34   0   7 772]
max_step 26 num_step 12
reward 0
(48,) 0 False {}
cur_obs [ 33   0   6   0   0 420  48  56  39   0  18 839]
max_step 26 num_step 13
reward 0
(48,) 0 False {}
cur_obs [ 17   0   2   0   0 450  30  47  19   0  28 876]
max_step 26 num_step 14
reward 0
(48,) 0 False {}
cur_obs [  8   0   3   0   0 480  11  36  11   0  28 914]
max_step 26 num_step 15
reward 0
(48,) 0 False {}
cur_obs [  4   0   5   0   0 510   5  18   9   0  24 944]
max_step 26 num_step 16
reward 0
(48,) 0 False {}
cur_obs [  2   0   2   0   0 540   2  12   4   0  22 960]
max_step 26 num_step 17
reward 0
(48,) 0 False {}
cur_obs [  2   0   0   0   0 570   1   5   2   0  19 973]
max_step 26 num_step 18
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 600   0   3   0   0  17 980]
max_step 26 num_step 19
reward 0
(48,) 0 False {}
cur_obs [  0   0   1   0   0 630   0   1   1   0  17 981]
max_step 26 num_step 20
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 660   0   0   0   0  17 983]
max_step 26 num_step 21
reward 0
(48,) 0 False {}
cur_obs [  0   0   0   0   0 690   0   0   0   0  17 983]
max_step 26<configparser.ConfigParser object at 0x7f9b043d30d0>
step 0
reward(train) tensor([[0.],
        [0.]])
0.0 0 0.0 0.0
step 1
reward(train) tensor([[0.],
        [0.]])
0.0 1 0.0 0.0
step 2
reward(train) tensor([[0.],
        [0.]])
0.0 2 0.0 0.0
step 3
reward(train) tensor([[0.],
        [0.]])
0.0 3 0.0 0.0
step 4
reward(train) tensor([[0.],
        [0.]])
0.0 4 0.0 0.0
step 5
reward(train) tensor([[0.],
        [0.]])
0.0 5 0.0 0.0
step 6
reward(train) tensor([[0.],
        [0.]])
0.0 6 0.0 0.0
step 7
reward(train) tensor([[0.],
        [0.]])
0.0 7 0.0 0.0
step 8
reward(train) tensor([[0.],
        [0.]])
0.0 8 0.0 0.0
step 9
reward(train) tensor([[0.],
        [0.]])
0.0 9 0.0 0.0
step 10
reward(train) tensor([[0.],
        [0.]])
0.0 10 0.0 0.0
step 11
reward(train) tensor([[0.],
        [0.]])
0.0 11 0.0 0.0
step 12
reward(train) tensor([[0.],
        [0.]])
0.0 12 0.0 0.0
step 13
reward(train) tensor([[0.],
        [0.]])
0.0 13 0.0 0.0
step 14
reward(train) tensor([[0.],
        [0.]])
0.0 14 0.0 0.0
step 15
reward(train) tensor([[0.],
        [0.]])
0.0 15 0.0 0.0
step 16
reward(train) tensor([[0.],
        [0.]])
0.0 16 0.0 0.0
step 17
reward(train) tensor([[0.],
        [0.]])
0.0 17 0.0 0.0
step 18
reward(train) tensor([[0.],
        [0.]])
0.0 18 0.0 0.0
step 19
reward(train) tensor([[0.],
        [0.]])
0.0 19 0.0 0.0
step 20
reward(train) tensor([[0.],
        [0.]])
0.0 20 0.0 0.0
step 21
reward(train) tensor([[0.],
        [0.]])
0.0 21 0.0 0.0
step 22
reward(train) tensor([[0.],
        [0.]])
0.0 22 0.0 0.0
step 23
reward(train) tensor([[0.],
        [0.]])
0.0 23 0.0 0.0
step 24
reward(train) tensor([[0.],
        [0.]])
0.0 24 0.0 0.0
step 25
reward(train) tensor([[0.],
        [0.]])
0.0 0 0
0.0 1 0
1.0 25 0.0 0.0
value_loss 1.9460	action_loss 0.1531	entropy 0.5265	total_loss 1.1209
step 0
reward(train) tensor([[0.],
        [0.]])
1.0 0 0.0 0.0
step 1
reward(train) tensor([[0.],
        [0.]])
1.0 1 0.0 0.0
step 2
reward(train) tensor([[0.],
        [0.]])
1.0 2 0.0 0.0
step 3
reward(train) tensor([[0.],
        [0.]])
1.0 3 0.0 0.0
step 4
reward(train) tensor([[0.],
        [0.]])
1.0 4 0.0 0.0
step 5
reward(train) tensor([[0.],
        [0.]])
1.0 5 0.0 0.0
step 6
reward(train) tensor([[0.],
        [0.]])
1.0 6 0.0 0.0
step 7
reward(train) tensor([[0.],
        [0.]])
1.0 7 0.0 0.0
step 8
reward(train) tensor([[0.],
        [0.]])
1.0 8 0.0 0.0
step 9
reward(train) tensor([[0.],
        [0.]])
1.0 9 0.0 0.0
step 10
reward(train) tensor([[0.],
        [0.]])
1.0 10 0.0 0.0
step 11
reward(train) tensor([[0.],
        [0.]])
1.0 11 0.0 0.0
step 12
reward(train) tensor([[0.],
        [0.]])
1.0 12 0.0 0.0
step 13
reward(train) tensor([[0.],
        [0.]])
1.0 13 0.0 0.0
step 14
reward(train) tensor([[0.],
        [0.]])
1.0 14 0.0 0.0
step 15
reward(train) tensor([[0.],
        [0.]])
1.0 15 0.0 0.0
step 16
reward(train) tensor([[0.],
        [0.]])
1.0 16 0.0 0.0
step 17
reward(train) tensor([[0.],
        [0.]])
1.0 17 0.0 0.0
step 18
reward(train) tensor([[0.],
        [0.]])
1.0 18 0.0 0.0
step 19
reward(train) tensor([[0.],
        [0.]])
1.0 19 0.0 0.0
step 20
reward(train) tensor([[0.],
        [0.]])
1.0 20 0.0 0.0
step 21
reward(train) tensor([[0.],
        [0.]])
1.0 21 0.0 0.0
step 22
reward(train) tensor([[0.],
        [0.]])
1.0 22 0.0 0.0
step 23
reward(train) tensor([[0.],
        [0.]])
1.0 23 0.0 0.0
step 24
reward(train) tensor([[0.],
        [0.]])
1.0 24 0.0 0.0
step 25
reward(train) tensor([[0.],
        [0.]])
1.0 0 0
1.0 1 0
2.0 25 0.0 0.0
value_loss 186.9008	action_loss 0.3375	entropy 0.6307	total_loss 93.7816
step 0
reward(train) tensor([[0.],
        [0.]])
2.0 0 0.0 0.0
step 1
reward(train) tensor([[0.],
        [0.]])
2.0 1 0.0 0.0
step 2
reward(train) tensor([[0.],
        [0.]])
2.0 2 0.0 0.0
step 3
reward(train) tensor([[0.],
        [0.]])
2.0 3 0.0 0.0
step 4
reward(train) tensor([[0.],
        [0.]])
2.0 4 0.0 0.0
step 5
reward(train) tensor([[0.],
        [0.]])
2.0 5 0.0 0.0
step 6
reward(train) tensor([[0.],
        [0.]])
2.0 6 0.0 0.0
step 7
reward(train) tensor([[0.],
        [0.]])
2.0 7 0.0 0.0
step 8
reward(train) tensor([[0.],
        [0.]])
2.0 8 0.0 0.0
step 9
reward(train) tensor([[0.],
        [0.]])
2.0 9 0.0 0.0
step 10
reward(train) tensor([[0.],
        [0.]])
2.0 10 0.0 0.0
step 11
reward(train) tensor([[0.],
        [0.]])
2.0 11 0.0 0.0
step 12
reward(train) tensor([[0.],
        [0.]])
2.0 12 0.0 0.0
step 13
reward(train) tensor([[0.],
        [0.]])
2.0 13 0.0 0.0
step 14
reward(train) tensor([[0.],
        [0.]])
2.0 14 0.0 0.0
step 15
reward(train) tensor([[0.],
        [0.]])
2.0 15 0.0 0.0
step 16
reward(train) tensor([[0.],
        [0.]])
2.0 16 0.0 0.0
step 17
reward(train) tensor([[0.],
        [0.]])
2.0 17 0.0 0.0
step 18
reward(train) tensor([[0.],
        [0.]])
2.0 18 0.0 0.0
step 19
reward(train) tensor([[0.],
        [0.]])
2.0 19 0.0 0.0
step 20
reward(train) tensor([[0.],
        [0.]])
2.0 20 0.0 0.0
step 21
reward(train) tensor([[0.],
        [0.]])
2.0 21 0.0 0.0
step 22
reward(train) tensor([[0.],
        [0.]])
2.0 22 0.0 0.0
step 23
reward(train) tensor([[0.],
        [0.]])
2.0 23 0.0 0.0
step 24
reward(train) tensor([[0.],
        [0.]])
2.0 24 0.0 0.0
step 25
reward(train) tensor([[0.],
        [0.]])
2.0 0 0
2.0 1 0
3.0 25 0.0 0.0
value_loss 142.1707	action_loss -1.7783	entropy 0.4457	total_loss 69.3026
